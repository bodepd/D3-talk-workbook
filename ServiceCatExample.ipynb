{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d537f533-55f7-467b-a774-55e6d204055c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /Users/253617/.config/pip/pip.conf\n"
     ]
    }
   ],
   "source": [
    "#install all of the packages\n",
    "!pip config set global.trusted-host \"pypi.org files.pythonhosted.org pypi.python.org\"\n",
    "!pip install python-dotenv >& /dev/null\n",
    "!pip install openai >& /dev/null\n",
    "!pip install --upgrade langchain >& /dev/null\n",
    "!pip install pyyaml >& /dev/null\n",
    "!pip install unstructured >& /dev/null\n",
    "!pip install markdown >& /dev/null\n",
    "!pip install chromadb >& /dev/null\n",
    "!pip install tiktoken >& /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dc4b89d-773e-4853-ad76-a9240aa6c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic setup\n",
    "\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "llm_model = 'gpt-3.5-turbo-0301'\n",
    "llm = ChatOpenAI(temperature=0.8, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a7413ee-8218-4d54-8413-29d1005a2d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4ea7d96-7656-496e-be2f-f0298c2ded48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   'account',\n",
      "    'authorization',\n",
      "    'workload',\n",
      "    'ingress',\n",
      "    'transitGateway',\n",
      "    'sqlDatabase',\n",
      "    'objectStorage']\n",
      "\n",
      "\n",
      "{   'account': 'Creates an AWS account that is secure and has entitlements for '\n",
      "               'authorization',\n",
      "    'authorization': 'creates groups and entitlements in active directoy that '\n",
      "                     'can be used for authorization',\n",
      "    'compute': 'Deploys containers together with everything required to '\n",
      "               'support upgrades and reduce downtime',\n",
      "    'ingress': 'Allows a deployed container to be routed to, deploys load '\n",
      "               'balancer, DNS address, certificates, and supports both '\n",
      "               'host-based routing and path-based routing',\n",
      "    'objectStorage': 'creates a directory where objects can be stored.',\n",
      "    'sqlDatabase': 'creates SQL databases',\n",
      "    'transitGatway': 'Provisions a hub network that can route between '\n",
      "                     'networks. Assigns ip addresses to avoid conflicts'}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Create all of the text strings that represent\n",
    "#\n",
    "\n",
    "import yaml\n",
    "\n",
    "with open('./service_catalogue/services.yaml', 'r') as file:\n",
    "    valid_services = yaml.safe_load(file)\n",
    "\n",
    "pp.pprint(valid_services)\n",
    "print(\"\\n\")\n",
    "\n",
    "with open('./service_catalogue/service_descriptions.yaml', 'r') as file:\n",
    "    descriptions = yaml.safe_load(file)\n",
    "\n",
    "pp.pprint(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27515e18-2500-4ee6-8128-880d95d7e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template that we can use to match queries to valid services from a service catalogue\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "service_matcher_template = \"\"\" You are a helpful assistant that helps developers get their applications\n",
    "running in the cloud.\n",
    "\n",
    "For the following text, determine which service from the list of valid_services the developer is asking about.\n",
    "\n",
    "valid_services: {valid_services}\n",
    "\n",
    "text: {text}\n",
    "\n",
    "The following json dictionary has each service as a key, and includes a high level\n",
    "description as a value. {descriptions}\n",
    "\n",
    "the output should be a list of valid servies being referenced\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "service_matcher_prompt_template = PromptTemplate.from_template(service_matcher_template)\n",
    "\n",
    "service_matcher_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt= service_matcher_prompt_template,\n",
    "    verbose=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eadd7529-29c8-4f25-b74b-433f59759301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['descriptions', 'text', 'valid_services']\n"
     ]
    }
   ],
   "source": [
    "# the prompt template is aware of the inputs that can be compiled into the template\n",
    "print(service_matcher_prompt_template.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20e34e3e-0c03-410a-8397-f64bb1abe65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The developer is asking about the 'ingress' service.\n"
     ]
    }
   ],
   "source": [
    "# let's run an example! here is a user question, let's see if our LLM can match what service it's referring to!\n",
    "messages = \"I want to route to my app using a dns address\"\n",
    "\n",
    "output = service_matcher_chain.predict(\n",
    "    descriptions=descriptions,\n",
    "    valid_services=valid_services,\n",
    "    text=messages,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f722596-6aaa-40c0-9fc6-e75a61e86022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our LLM got the service right, but the format didn't return as we requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04f3e670-99d9-425b-99fb-930f31c0a5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"ingress\"]\n"
     ]
    }
   ],
   "source": [
    "# We can pass the output back to the LLM and ask it again to return into a format that we\n",
    "# can interact with programmatically\n",
    "\n",
    "format_fixer_template_string = \"\"\" Take the text as input\n",
    "\n",
    "Convert the text that describes a list of services into a list of service in json\n",
    "\n",
    "text: the service is compute\n",
    "\n",
    "[\"compute\"]\n",
    "\n",
    "text: {text}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "format_fixer_prompt_template = PromptTemplate.from_template(format_fixer_template_string) \n",
    "\n",
    "format_correcting_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt= format_fixer_prompt_template,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "output = format_correcting_chain.predict(\n",
    "    text=output,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2441541-5bf9-4091-98a0-4086b639784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We not only want to understand which service the user is referring to, but also what their intent is.\n",
    "# As mentioned before, we want to take the combination of intent + service so that our code can\n",
    "# know what steps to perform\n",
    "\n",
    "user_intent_template_str = \"\"\" You are a helpful assistant that helps developers get their applications\n",
    "running in the cloud.\n",
    "\n",
    "For the following text, determine if the user is asking a question, trying to provision a resource, or reporting an issue\n",
    "\n",
    "the format of the response should be one of the following strings, \"Asking a Question\", \"Reporting an issue\", \"Provisioning a resource\"\" \n",
    "\n",
    "text: {text}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_intent_template = PromptTemplate.from_template(user_intent_template_str) \n",
    "\n",
    "user_intent_llm_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt= user_intent_template,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd3f670d-46ee-4de2-83cc-0140db702369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting an issue\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Let's see if our LLM can determine our intent here\n",
    "#\n",
    "message2 = \"My database failed\"\n",
    "output = user_intent_llm_chain.predict(\n",
    "    text=message2,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13f6a789-ad1a-48b1-8921-e3eafd22743e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The service being referenced in the text is 'ingress'.\n",
      "Provisioning a resource\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Let's try to figure out both the intent of a query and the service it relates to\n",
    "#\n",
    "\n",
    "message2 = \"I want to deploy my container\"\n",
    "\n",
    "intent_output = user_intent_llm_chain.predict(\n",
    "    text=message2,\n",
    ")\n",
    "service_type_output = service_matcher_chain.predict(\n",
    "    descriptions=descriptions,\n",
    "    valid_services=valid_services,\n",
    "    text=messages,\n",
    ")\n",
    "print(service_type_output)\n",
    "print(intent_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0687ee8e-5006-41d8-8d0c-235681717bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: it does not pick the right service, we can try to see if we can use embeddings to do a better job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77e11b24-6aef-44c2-b097-873e8acc68ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Header 1': 'Overview', 'Document': 'account.md'}\n",
      "{'Header 1': 'Inputs', 'Document': 'account.md'}\n",
      "{'Header 1': 'Outputs', 'Document': 'account.md'}\n",
      "{'Header 1': 'Overview', 'Document': 'authorization.md'}\n",
      "{'Header 1': 'Overview', 'Document': 'workload.md'}\n",
      "{'Header 1': 'Inputs', 'Document': 'workload.md'}\n",
      "{'Header 1': 'outputs', 'Document': 'workload.md'}\n",
      "{'Header 1': 'Overview', 'Document': 'ingress.md'}\n",
      "{'Header 1': 'inputs', 'Document': 'ingress.md'}\n",
      "{'Header 1': 'Overview', 'Document': 'transitGateway.md'}\n",
      "{'Header 1': 'Inputs', 'Document': 'transitGateway.md'}\n",
      "{'Header 1': 'Overview', 'Document': 'objectStorage.md'}\n",
      "{'Header 1': 'inputs', 'Document': 'objectStorage.md'}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Split up our markdown documentation by splitting it up per section\n",
    "#\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "\n",
    "split_sections = []\n",
    "for s in valid_services:\n",
    "    with open(f'./service_catalogue/{s}.md', 'r') as file:\n",
    "        read_file = file.read()\n",
    "        md_header_splits = markdown_splitter.split_text(read_file)\n",
    "        for i in md_header_splits:\n",
    "            i.metadata['Document'] = f'{s}.md'\n",
    "            print(i.metadata)\n",
    "            split_sections.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "381589e5-9e99-4eca-b47d-8a5bcfa58eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "[-0.003914595988357722, -0.014495498282681086, 0.013440788521313102, -0.005354680506045646]\n",
      "1536\n",
      "[0.01918765961014604, -0.014656718335834696, -0.013342495483116794, 0.008755230611743351]\n"
     ]
    }
   ],
   "source": [
    "# We can get a vector representation\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "punchlines = embeddings_model.embed_documents(\n",
    "    \"To get to the other side\",\n",
    "    \"You forgot to say banana\",\n",
    ")\n",
    "print(len(punchlines[0]))\n",
    "print(punchlines[0][:4])\n",
    "set_ups = embeddings_model.embed_query(\n",
    "    \"Why did the chicken cross the road?\"\n",
    ")\n",
    "print(len(set_ups))\n",
    "print(set_ups[:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a560138d-9dd8-4a6f-ac3a-8ff60948407d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# Build a vectordb for our embeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'docs/chroma/'\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    persist_directory=persist_directory,\n",
    "    documents=split_sections,\n",
    "    embedding=embeddings_model,\n",
    ")\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4de7508c-fb46-4a52-a986-37e99ee96c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p docs/chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a3009b7-4794-48c1-95e3-3bec40bd8e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "# Build a vectordb for our embeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'docs/chroma/'\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    persist_directory=persist_directory,\n",
    "    documents=split_sections,\n",
    "    embedding=embeddings_model,\n",
    ")\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58196a2b-4f7e-46b5-9f0e-12e8b8e8b025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain.vectorstores.chroma.Chroma'>\n"
     ]
    }
   ],
   "source": [
    "print(type(vectordb))\n",
    "question = \"How do I ensure there are no ip conflicts?\"\n",
    "docs = vectordb.similarity_search(question,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "674a1fcb-6725-41a7-8ab3-b94959eaec28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   Document(page_content='The transit gateway services allows the provisioning of a network that\\nallows you to route from the VPC in your cloud account to other VPCs.  \\nIt does this both by configuring routes from your VPC to the transit\\nnewtwork and also by configuring DNS in your account so that you can\\nboth resolve DNS addresses across the network and so that you can\\nleverage DNS addresses that can be resolved.  \\nThe transit gateway also assigns you ip addresses using IPAM to ensure that\\nyou do not get an overlapping IP.', metadata={'Document': 'transitGateway.md', 'Header 1': 'Overview'}),\n",
      "    Document(page_content='The transit gateway services allows the provisioning of a network that\\nallows you to route from the VPC in your cloud account to other VPCs.  \\nIt does this both by configuring routes from your VPC to the transit\\nnewtwork and also by configuring DNS in your account so that you can\\nboth resolve DNS addresses across the network and so that you can\\nleverage DNS addresses that can be resolved.  \\nThe transit gateway also assigns you ip addresses using IPAM to ensure that\\nyou do not get an overlapping IP.', metadata={'Document': 'transitGateway.md', 'Header 1': 'Overview'}),\n",
      "    Document(page_content='hostname: Each provisioned deployment gets a local hostname', metadata={'Document': 'workload.md', 'Header 1': 'outputs'}),\n",
      "    Document(page_content='hostname: Each provisioned deployment gets a local hostname', metadata={'Document': 'workload.md', 'Header 1': 'outputs'}),\n",
      "    Document(page_content='the object store configures S3 buckets in a way that is secure.', metadata={'Document': 'objectStorage.md', 'Header 1': 'Overview'})]\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f13c548-3f9d-4cb4-9fac-37c24084acdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workload.md\n",
      "{'transitGateway.md': 1, 'workload.md': 4}\n"
     ]
    }
   ],
   "source": [
    "# Performing a similarity search on the embeddings from the markdown is far more effective for matching the service than\n",
    "# above prompt.\n",
    "# You can see that we have the most matches from our compute document, but also that the best match is from that document\n",
    "question2 = \"I want to deploy my container\"\n",
    "docs = vectordb.similarity_search(question2,k=5)\n",
    "\n",
    "# which service from the service catalogue had the best match?\n",
    "\n",
    "def get_best_matching_document(docs):\n",
    "    return docs[0].metadata['Document']\n",
    "\n",
    "print(get_best_matching_document(docs))\n",
    "\n",
    "# which service catalogue document got the most matches?\n",
    "\n",
    "def get_most_matching_document(docs):\n",
    "    match_dict = {}\n",
    "    for i in docs:\n",
    "        doc_name = i.metadata['Document']\n",
    "        if match_dict.get(doc_name):\n",
    "            match_dict[doc_name] = match_dict[doc_name] + 1\n",
    "        else:\n",
    "            match_dict[doc_name] = 1 \n",
    "    return match_dict\n",
    "\n",
    "\n",
    "pp.pprint(get_most_matching_document(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4be2800-a1e0-4c12-99cc-6625d553eb5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './service_catalogue/compute.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# determine service and what the user wants, if they want to deploy a service, generate the spec\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./service_catalogue/compute.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      6\u001b[0m     compute_spec \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(file)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_spec\u001b[39m(spec):\n",
      "File \u001b[0;32m~/working/ai/d3_talk/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './service_catalogue/compute.yaml'"
     ]
    }
   ],
   "source": [
    "#\n",
    "# determine service and what the user wants, if they want to deploy a service, generate the spec\n",
    "#\n",
    "\n",
    "with open('./service_catalogue/compute.yaml', 'r') as file:\n",
    "    compute_spec = yaml.safe_load(file)\n",
    "\n",
    "def generate_spec(spec):\n",
    "    spec_generate_string = \"\"\" You are a helpful assistant that helps developers generate the declarative specifications to get their resources running in the cloud.\n",
    "        Generate a specicifcation that matches the following:\n",
    "        {spec}\n",
    "    \"\"\"\n",
    "    user_spec_template = PromptTemplate.from_template(spec_generate_string)\n",
    "    user_spec_chain = LLMChain(\n",
    "        llm = llm,\n",
    "        prompt= user_spec_template,\n",
    "        verbose=True,\n",
    "    )\n",
    "    return user_spec_chain.predict(spec=spec)\n",
    "\n",
    "message = \"I want to deploy my container\"\n",
    "intent = user_intent_llm_chain.predict(\n",
    "    text=message,\n",
    ")\n",
    "docs = vectordb.similarity_search(message,k=5)\n",
    "service = get_best_matching_document(docs)\n",
    "print(intent)\n",
    "print(service)\n",
    "\n",
    "if intent == 'Provisioning a resource' and service == 'compute.md':\n",
    "    output = generate_spec(compute_spec)\n",
    "    print(output)\n",
    "\n",
    "# it ignores my spec and generates a K8s spec!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b2539-8e8b-4da5-b1f6-826a56a729a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "llm_zero = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm_zero,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result['result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3772147-beb9-45bc-a6bd-75697b54e7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# if the user wants to report an issue, open a service now ticket on their behalf\n",
    "#\n",
    "\n",
    "message = \"My SQL DB failed\"\n",
    "intent = user_intent_llm_chain.predict(\n",
    "    text=message,\n",
    ")\n",
    "docs = vectordb.similarity_search(message,k=5)\n",
    "service = get_best_matching_document(docs)\n",
    "print(intent)\n",
    "print(service)\n",
    "print(get_most_matching_document(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e815cd24-96d3-4fff-af56-beb9e3a74211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why is is wrong?\n",
    "\n",
    "with open('./service_catalogue/sqlDatabase.md', 'r') as file:\n",
    "    print(file.read())\n",
    "\n",
    "# I made up all of the docs, so I never filled the sqldatabase one in, so text, no embedding that are similar to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f58f47-93a8-43c9-8725-6ddf84dc8904",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"I can't access my buckets\"\n",
    "intent = user_intent_llm_chain.predict(\n",
    "    text=message,\n",
    ")\n",
    "docs = vectordb.similarity_search(message,k=5)\n",
    "service = get_best_matching_document(docs)\n",
    "print(intent)\n",
    "print(service)\n",
    "print(get_most_matching_document(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ef9ce-6c3e-4299-8751-a980c24368d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.python import PythonREPL\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.agents import tool\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "zero_llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "\n",
    "def submit_service_now_ticket(service: str, query: str) -> str:\n",
    "    \"\"\"Calls out to service now to create a new ticket for the specific service passed in.\"\"\"\n",
    "    print(f'!!!!!submitting ticket to service now for {service}')\n",
    "\n",
    "tool = StructuredTool.from_function(submit_service_now_ticket)\n",
    "\n",
    "#agent = create_python_agent(\n",
    "#    zero_llm,\n",
    "#    tool=PythonREPLTool(),\n",
    "#    verbose=True\n",
    "#)\n",
    "\n",
    "agent= initialize_agent(\n",
    "    [tool], \n",
    "    zero_llm, \n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = agent(f'Submit a ticket to service now with the following content {message} for the service {service}') \n",
    "    #print(results)\n",
    "except: \n",
    "    print(\"exception on external access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e2f19-0df8-4912-9fc8-bfc6cd5e061f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
