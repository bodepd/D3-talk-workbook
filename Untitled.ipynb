{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d537f533-55f7-467b-a774-55e6d204055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip config set global.trusted-host \"pypi.org files.pythonhosted.org pypi.python.org\"\n",
    "!pip install python-dotenv\n",
    "!pip install openai\n",
    "!pip install --upgrade langchain\n",
    "!pip install pyyaml\n",
    "!pip install unstructured\n",
    "!pip install markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b92f1d9-93db-4fa9-9c75-f637c58d1550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc4b89d-773e-4853-ad76-a9240aa6c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eb2964-9705-4b47-8307-21b1b7e2300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.8, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea7d96-7656-496e-be2f-f0298c2ded48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('./service_catalogue/services.yaml', 'r') as file:\n",
    "    valid_services = yaml.safe_load(file)\n",
    "\n",
    "with open('./service_catalogue/service_descriptions.yaml', 'r') as file:\n",
    "    descriptions = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27515e18-2500-4ee6-8128-880d95d7e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "template_string = \"\"\" You are a helpful assistant that helps developers get their applications\n",
    "running in the cloud.\n",
    "\n",
    "For the following text, determine which service from the list of valid_services the developer is asking about.\n",
    "\n",
    "valid_services: {valid_services}\n",
    "\n",
    "text: {text}\n",
    "\n",
    "The following json dictionary has each service as a key, and includes a high level\n",
    "description as a value. {descriptions}\n",
    "\n",
    "the output should be a list of valid servies being referenced\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template_string) \n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt= prompt_template,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2a7533-871e-496c-bf50-1813ae72ee9c",
   "metadata": {},
   "source": [
    "template_string = \"\"\"\n",
    "You are a helpful assistant that will help developers get their application \\\n",
    "running in the cloud.\n",
    "\n",
    "For the following text, determine which service from the list of valid_services the developer is asking about.\n",
    "\n",
    "valid_services: {services}\n",
    "\n",
    "text: {text}\n",
    "\n",
    "The following json dictionary has each service as a key, and includes a high level \\\n",
    "description as a value. {descriptions} \\\n",
    "\n",
    "format the output as a list of services.\n",
    "\n",
    "If you think that the user is not asking a question about running an application in the cloud, return nonsense as the service name\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd7529-29c8-4f25-b74b-433f59759301",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_template.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457c2829-768c-4e3f-bf33-211a91065413",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = \"I want to route to my app using a dns address\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e34e3e-0c03-410a-8397-f64bb1abe65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm_chain.predict(\n",
    "    descriptions=descriptions,\n",
    "    valid_services=valid_services,\n",
    "    text=messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c78d30-d8d5-4ec3-8862-aa5b7471693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f3e670-99d9-425b-99fb-930f31c0a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "format_fixer_template_string = \"\"\" Take the text as input\n",
    "\n",
    "Convert the text that describes a list of services into a list of service in json\n",
    "\n",
    "text: the service is compute\n",
    "\n",
    "[\"compute\"]\n",
    "\n",
    "text: {text}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "format_fixer_prompt_template = PromptTemplate.from_template(format_fixer_template_string) \n",
    "\n",
    "format_correcting_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt= format_fixer_prompt_template,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "output = format_correcting_chain.predict(\n",
    "    text=output,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9980f0f5-3092-4723-bf74-d88c2f21e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e11b24-6aef-44c2-b097-873e8acc68ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "\n",
    "split_sections = []\n",
    "for s in valid_services:\n",
    "    with open(f'./service_catalogue/{s}.md', 'r') as file:\n",
    "        read_file = file.read()\n",
    "        md_header_splits = markdown_splitter.split_text(read_file)\n",
    "        for i in md_header_splits:\n",
    "            i.metadata['Document'] = f'{s}.md'\n",
    "            print(i.metadata)\n",
    "            split_sections.append(i)\n",
    "\n",
    "print(len(split_sections))\n",
    "print(type(split_sections[0]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a560138d-9dd8-4a6f-ac3a-8ff60948407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's build an embedding based on openai embeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# persist_directory = 'docs/chroma/'\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(\n",
    "    #persist_directory=persist_directory,\n",
    "    documents=split_sections,\n",
    "    embedding=embedding,\n",
    ")\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58196a2b-4f7e-46b5-9f0e-12e8b8e8b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do I ensure there are no ip conflicts?\"\n",
    "docs = vectordb.similarity_search(question,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674a1fcb-6725-41a7-8ab3-b94959eaec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b2539-8e8b-4da5-b1f6-826a56a729a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "llm_zero = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm_zero,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b62d03-d14e-4e61-8f07-04ab23fbf6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5addcc-7dca-4f3c-8694-f3a6ce6f163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8cd4c-cc57-451b-b7ac-518bfb7ced4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out if the user wants to deploy\n",
    "# what the resource is\n",
    "# a resource, and if so, deploy it for them using python codegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3772147-beb9-45bc-a6bd-75697b54e7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
